{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38679b81",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0af5d",
   "metadata": {},
   "source": [
    "## Load Transcript Data\n",
    "\n",
    "Load the transcripts generated by Whisper from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b15d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 33 good answers\n",
      "âŒ Loaded 13 bad answers\n",
      "\n",
      "ğŸ“Š Total samples: 46\n"
     ]
    }
   ],
   "source": [
    "def load_transcripts(folder_path, label):\n",
    "    \"\"\"\n",
    "    Load all transcript files from a folder.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to folder containing transcript .txt files\n",
    "        label: 0 for bad, 1 for good\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with 'text' and 'label' keys\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    if not folder.exists():\n",
    "        print(f\"âš ï¸  Folder not found: {folder_path}\")\n",
    "        return []\n",
    "    \n",
    "    data = []\n",
    "    txt_files = sorted(folder.glob(\"*.txt\"))\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        try:\n",
    "            text = txt_file.read_text(encoding='utf-8').strip()\n",
    "            if text:  # Skip empty files\n",
    "                data.append({\n",
    "                    'text': text,\n",
    "                    'label': label\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error reading {txt_file.name}: {e}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load good answers (label=1)\n",
    "good_data = load_transcripts(\"../data/good_answer_transcripts\", label=1)\n",
    "print(f\"âœ… Loaded {len(good_data)} good answers\")\n",
    "\n",
    "# Load bad answers (label=0)\n",
    "bad_data = load_transcripts(\"../data/bad_answer_transcripts\", label=0)\n",
    "print(f\"âŒ Loaded {len(bad_data)} bad answers\")\n",
    "\n",
    "# Combine datasets\n",
    "all_data = good_data + bad_data\n",
    "print(f\"\\nğŸ“Š Total samples: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd24c9-861d-4fad-aa47-8273832ac8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>×–×” ×‘×¢×¦× ×œ× ×™×—×•×’, ×¦×¨×™×š ×œ×”×‘×™×Ÿ ×§×•×“× ×‘×¢×–×—×•×’ ××ª ×”××ª...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>××•×§×™×™, ××ª×” ×¨×•×¦×” ×œ×”×ª×§×‘×œ ×œ×ª×•××¨ ×”×¨××©×•×Ÿ ×‘×”× ×“×¡×” ×‘×™×¨...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>××”×‘×—×™× ×ª ××ª ×“××¨×™×©×•× ××– ×‘×•× ×¨×× ×‘×¢×¦× ×™×© ×›××œ×” ×©×¤×ª×•...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>×—×œ×§ ××”×˜×™×™××”, ××•× ×™×‘×¨×¡×™×˜××™× ×œ×˜×•×¨ ×©×•×Ÿ ×”× ×‘×¢×¦× ×™×“×¢...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>×‘×—×œ×§ ××”××›×•×’×™× ×‘×¤×§×•×œ×˜×•×ª ×œ××“×¢×¨ ×”×•× ××—×¨ ××• ×‘×¤×§×•×œ×˜...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  ×–×” ×‘×¢×¦× ×œ× ×™×—×•×’, ×¦×¨×™×š ×œ×”×‘×™×Ÿ ×§×•×“× ×‘×¢×–×—×•×’ ××ª ×”××ª...      1\n",
       "1  ××•×§×™×™, ××ª×” ×¨×•×¦×” ×œ×”×ª×§×‘×œ ×œ×ª×•××¨ ×”×¨××©×•×Ÿ ×‘×”× ×“×¡×” ×‘×™×¨...      1\n",
       "2  ××”×‘×—×™× ×ª ××ª ×“××¨×™×©×•× ××– ×‘×•× ×¨×× ×‘×¢×¦× ×™×© ×›××œ×” ×©×¤×ª×•...      1\n",
       "3  ×—×œ×§ ××”×˜×™×™××”, ××•× ×™×‘×¨×¡×™×˜××™× ×œ×˜×•×¨ ×©×•×Ÿ ×”× ×‘×¢×¦× ×™×“×¢...      1\n",
       "4  ×‘×—×œ×§ ××”××›×•×’×™× ×‘×¤×§×•×œ×˜×•×ª ×œ××“×¢×¨ ×”×•× ××—×¨ ××• ×‘×¤×§×•×œ×˜...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85e4bc",
   "metadata": {},
   "source": [
    "## Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    logits = pred.predictions\n",
    "\n",
    "    preds = logits.argmax(-1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    # AUC needs probabilities for class 1\n",
    "    probs_pos = np.exp(logits) / np.exp(logits).sum(axis=1, keepdims=True)\n",
    "    auc = roc_auc_score(labels, probs_pos[:, 1])\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall, \"auc\": auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa80103-d6f7-4592-bfc3-e9d10e1b2c53",
   "metadata": {},
   "source": [
    "## Check Class Distribution and Setup Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1ad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Random Search Configuration\n",
      "==================================================\n",
      "Model: onlplab/alephbert-base\n",
      "Total possible combinations: 162\n",
      "Random samples to try: 20\n",
      "CV folds per sample: 5\n",
      "Total training runs: 100\n",
      "Coverage: 12.3% of search space\n",
      "\n",
      "Parameter search space:\n",
      "  learning_rate: [1e-05, 2e-05, 3e-05]\n",
      "  num_train_epochs: [15, 20, 25]\n",
      "  per_device_train_batch_size: [4, 8]\n",
      "  weight_decay: [0.01, 0.02, 0.05]\n",
      "  warmup_ratio: [0.0, 0.1, 0.2]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter search space\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-5, 2e-5, 3e-5],\n",
    "    'num_train_epochs': [15, 20, 25],\n",
    "    'per_device_train_batch_size': [4, 8],\n",
    "    'weight_decay': [0.01, 0.02, 0.05],\n",
    "    'warmup_ratio': [0.0, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "# Model to use\n",
    "model_names = [\n",
    "    \"onlplab/alephbert-base\",\n",
    "]\n",
    "\n",
    "# Random search configuration\n",
    "total_combinations = int(np.prod([len(v) for v in param_grid.values()]))\n",
    "n_random_samples = 20  # Try 20 random combinations instead of all 162\n",
    "n_folds = 5\n",
    "\n",
    "print(\"Hyperparameter Random Search Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {model_names[0]}\")\n",
    "print(f\"Total possible combinations: {total_combinations}\")\n",
    "print(f\"Random samples to try: {n_random_samples}\")\n",
    "print(f\"CV folds per sample: {n_folds}\")\n",
    "print(f\"Total training runs: {n_random_samples * n_folds}\")\n",
    "print(f\"Coverage: {n_random_samples/total_combinations*100:.1f}% of search space\")\n",
    "print(\"\\nParameter search space:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d778e6",
   "metadata": {},
   "source": [
    "## Cross-Validation Training Function\n",
    "\n",
    "Function to train model with k-fold cross-validation and return mean AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d7a20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cross-validation function defined\n"
     ]
    }
   ],
   "source": [
    "def train_with_cv(model_name, params, df, n_splits=5):\n",
    "    \"\"\"\n",
    "    Train model with stratified k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model name\n",
    "        params: Dictionary of hyperparameters\n",
    "        df: DataFrame with 'text' and 'label' columns\n",
    "        n_splits: Number of CV folds\n",
    "    \n",
    "    Returns:\n",
    "        mean_auc: Average AUC across folds\n",
    "        std_auc: Standard deviation of AUC\n",
    "        fold_aucs: List of AUC scores for each fold\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_aucs = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df['text'], df['label'])):\n",
    "        print(f\"  Fold {fold + 1}/{n_splits}\", end=\" \")\n",
    "        \n",
    "        # Split data\n",
    "        train_texts = df.iloc[train_idx]['text'].tolist()\n",
    "        train_labels = df.iloc[train_idx]['label'].tolist()\n",
    "        val_texts = df.iloc[val_idx]['text'].tolist()\n",
    "        val_labels = df.iloc[val_idx]['label'].tolist()\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "        val_dataset = Dataset.from_dict({'text': val_texts, 'label': val_labels})\n",
    "        \n",
    "        # Tokenize\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples['text'], truncation=True, max_length=512)\n",
    "        \n",
    "        train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "        val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "        \n",
    "        train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "        val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"../models/cv_temp_fold_{fold}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",  # Don't save during CV\n",
    "            learning_rate=params['learning_rate'],\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            warmup_ratio=params['warmup_ratio'],\n",
    "            per_device_train_batch_size=params['per_device_train_batch_size'],\n",
    "            per_device_eval_batch_size=16,\n",
    "            gradient_accumulation_steps=4,\n",
    "            num_train_epochs=params['num_train_epochs'],\n",
    "            weight_decay=params['weight_decay'],\n",
    "            load_best_model_at_end=False,\n",
    "            metric_for_best_model=\"auc\",\n",
    "            greater_is_better=True,\n",
    "            logging_steps=1000,  # Reduce logging during CV\n",
    "            report_to=\"none\",\n",
    "            seed=42 + fold,\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "        \n",
    "        # Trainer with early stopping\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        trainer.train()\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = trainer.evaluate()\n",
    "        fold_auc = eval_result['eval_auc']\n",
    "        fold_aucs.append(fold_auc)\n",
    "        \n",
    "        print(f\"â†’ AUC: {fold_auc:.4f}\")\n",
    "        \n",
    "        # Clean up memory\n",
    "        del model, trainer, tokenizer\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    mean_auc = np.mean(fold_aucs)\n",
    "    std_auc = np.std(fold_aucs)\n",
    "    \n",
    "    return mean_auc, std_auc, fold_aucs\n",
    "\n",
    "print(\"âœ… Cross-validation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b12c11",
   "metadata": {},
   "source": [
    "## Run Random Hyperparameter Search with Cross-Validation\n",
    "\n",
    "Randomly sample parameter combinations to efficiently explore the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38682579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING RANDOM HYPERPARAMETER SEARCH WITH CV\n",
      "============================================================\n",
      "Objective: Maximize AUC\n",
      "\n",
      "\n",
      "============================================================\n",
      "MODEL 1/1: onlplab/alephbert-base\n",
      "============================================================\n",
      "\n",
      "[Config 1/20] LR=3e-05, Epochs=15, BS=4, WD=0.05, Warmup=0.0\n",
      "  Fold 1/5 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569cc231276948dfb1c38c1ce8a3d4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e5fb27c68b4cc691f65b0dfae1b82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/45 02:32 < 08:24, 0.07 it/s, Epoch 4/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.688668</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.904762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.646711</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.513045</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.904762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.467791</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.904762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ AUC: 0.9048\n",
      "  Fold 2/5 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2ad9f7ad334b7fac33f63f6c0d5da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f20790daaa48ce91e3d19abb5b0400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/45 02:56 < 09:43, 0.06 it/s, Epoch 4/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.475296</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.439727</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.453083</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.296029</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ AUC: 1.0000\n",
      "  Fold 3/5 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398a97e85470461f98be18feeeeb54a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e36a2aa27c4ddd80700f59776b27a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/45 03:25 < 11:17, 0.05 it/s, Epoch 4/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.482661</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.434657</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.354652</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.270993</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ AUC: 1.0000\n",
      "  Fold 4/5 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9840bf80cd0c43b2b54fee49d446fc16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fb33cc1a71414cb58d8c0c498afba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/45 03:34 < 11:46, 0.05 it/s, Epoch 4/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.834348</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.673459</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.558438</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.529937</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ AUC: 0.8333\n",
      "  Fold 5/5 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6874cdfeb343d8b8d1a3273c4a9015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b78ce1d8f0d4f83b11db02e5021dc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/45 03:19 < 10:58, 0.05 it/s, Epoch 4/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.718709</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.615637</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.665690</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.821665</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ AUC: 0.7778\n",
      "  âœ Mean AUC: 0.9032 Â± 0.0887\n",
      "  ğŸ† NEW BEST AUC!\n",
      "[Config 2/20] LR=1e-05, Epochs=25, BS=8, WD=0.05, Warmup=0.2\n",
      "  Fold 1/5 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f839e03fc694adea84323239712be5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283a78db74004acfaf13b82c33dd21d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/50 13:28 < 17:19, 0.03 it/s, Epoch 11/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.663864</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.646697</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.629840</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.623983</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.619048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.626557</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.621336</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.606873</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.588356</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.573347</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.562868</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.553491</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run random search with cross-validation\n",
    "results = []\n",
    "best_auc = 0\n",
    "best_config = None\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING RANDOM HYPERPARAMETER SEARCH WITH CV\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Objective: Maximize AUC\\n\")\n",
    "\n",
    "for model_idx, model_name in enumerate(model_names, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MODEL {model_idx}/{len(model_names)}: {model_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Generate random parameter combinations\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    tested_configs = set()  # Avoid duplicates\n",
    "    param_combinations = []\n",
    "    \n",
    "    while len(param_combinations) < n_random_samples:\n",
    "        # Randomly sample one value from each parameter\n",
    "        random_params = {\n",
    "            param: np.random.choice(values)\n",
    "            for param, values in param_grid.items()\n",
    "        }\n",
    "        \n",
    "        # Convert numpy types to native Python types\n",
    "        random_params = {\n",
    "            k: int(v) if isinstance(v, (np.integer, np.int64, np.int32)) else \n",
    "               float(v) if isinstance(v, (np.floating, np.float64, np.float32)) else v\n",
    "            for k, v in random_params.items()\n",
    "        }\n",
    "        \n",
    "        # Convert to hashable format to check for duplicates\n",
    "        config_hash = tuple(sorted(random_params.items()))\n",
    "        if config_hash not in tested_configs:\n",
    "            tested_configs.add(config_hash)\n",
    "            param_combinations.append(random_params)\n",
    "    \n",
    "    for i, params in enumerate(param_combinations, 1):\n",
    "        print(f\"[Config {i}/{n_random_samples}] \", end=\"\")\n",
    "        print(f\"LR={params['learning_rate']}, Epochs={params['num_train_epochs']}, \", end=\"\")\n",
    "        print(f\"BS={params['per_device_train_batch_size']}, WD={params['weight_decay']}, \", end=\"\")\n",
    "        print(f\"Warmup={params['warmup_ratio']}\")\n",
    "        \n",
    "        try:\n",
    "            mean_auc, std_auc, fold_aucs = train_with_cv(model_name, params, df, n_splits=5)\n",
    "            \n",
    "            result = {\n",
    "                'model_name': model_name,\n",
    "                'params': params,\n",
    "                'mean_auc': mean_auc,\n",
    "                'std_auc': std_auc,\n",
    "                'fold_aucs': fold_aucs\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"  âœ Mean AUC: {mean_auc:.4f} Â± {std_auc:.4f}\")\n",
    "            \n",
    "            # Track best configuration\n",
    "            if mean_auc > best_auc:\n",
    "                best_auc = mean_auc\n",
    "                best_config = result\n",
    "                print(f\"  ğŸ† NEW BEST AUC!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error: {e}\")\n",
    "            continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RANDOM SEARCH COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Tested {len(results)}/{n_random_samples} configurations\")\n",
    "print(f\"Best AUC: {best_auc:.4f}\")\n",
    "print(f\"Best model: {best_config['model_name']}\")\n",
    "print(\"\\nBest parameters:\")\n",
    "for k, v in best_config['params'].items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014849f5",
   "metadata": {},
   "source": [
    "## Prepare Data and Train Final Model with Best Configuration\n",
    "\n",
    "Now split data 80/20 and train final model using the best hyperparameters found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5cc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and tokenizer\n",
    "model_name = best_config['model_name']\n",
    "best_params = best_config['params']\n",
    "\n",
    "# Ensure all params are native Python types (not numpy)\n",
    "best_params = {\n",
    "    k: int(v) if isinstance(v, (np.integer, np.int64, np.int32)) else \n",
    "       float(v) if isinstance(v, (np.floating, np.float64, np.float32)) else v\n",
    "    for k, v in best_params.items()\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "print(f\"Loaded {model_name}\")\n",
    "print(f\"\\nBest parameters from CV (AUC: {best_config['mean_auc']:.4f} Â± {best_config['std_auc']:.4f}):\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Split data (80/20)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'].tolist(),\n",
    "    df['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_texts)} samples\")\n",
    "print(f\"Validation set: {len(val_texts)} samples\")\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'text': train_texts,\n",
    "    'label': train_labels\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'text': val_texts,\n",
    "    'label': val_labels\n",
    "})\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=512)\n",
    "\n",
    "# Apply tokenization\n",
    "print(\"\\nTokenizing datasets...\")\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(\"âœ… Tokenization complete!\")\n",
    "\n",
    "# Training arguments using best parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/customer-support-classifier\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=best_params['warmup_ratio'],\n",
    "    per_device_train_batch_size=best_params['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=best_params['num_train_epochs'],\n",
    "    weight_decay=best_params['weight_decay'],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"auc\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"../models/logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training configuration ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc636daf",
   "metadata": {},
   "source": [
    "## Initialize Trainer with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Initialize Trainer with early stopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer initialized with early stopping!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c68282",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "This may take several minutes depending on hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\\n\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Train loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9066dd",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on validation set...\\n\")\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {eval_result['eval_accuracy']:.4f}\")\n",
    "print(f\"Precision: {eval_result['eval_precision']:.4f}\")\n",
    "print(f\"Recall:    {eval_result['eval_recall']:.4f}\")\n",
    "print(f\"F1 Score:  {eval_result['eval_f1']:.4f}\")\n",
    "print(f\"Loss:      {eval_result['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a07c14",
   "metadata": {},
   "source": [
    "## Save Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "output_dir = \"../models/customer-support-classifier-final\"\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Saving model to: {output_dir}\")\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"\\nâœ… Model saved successfully!\")\n",
    "print(f\"\\nTo load this model later:\")\n",
    "print(f\"  tokenizer = AutoTokenizer.from_pretrained('{output_dir}')\")\n",
    "print(f\"  model = AutoModelForSequenceClassification.from_pretrained('{output_dir}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15a38af",
   "metadata": {},
   "source": [
    "## Test Prediction\n",
    "\n",
    "Try the model on a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ae205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_quality(text):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    m = trainer.model.to(device)\n",
    "    m.eval()\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = m(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
    "        predicted_class = int(torch.argmax(probs).item())\n",
    "        confidence = float(probs[predicted_class].item())\n",
    "\n",
    "    return {\n",
    "        \"prediction\": \"Good Answer\" if predicted_class == 1 else \"Bad Answer\",\n",
    "        \"confidence\": confidence,\n",
    "        \"class\": predicted_class\n",
    "    }\n",
    "\n",
    "\n",
    "# Test on a validation example\n",
    "if len(val_texts) > 0:\n",
    "    test_text = val_texts[0]\n",
    "    result = predict_quality(test_text)\n",
    "    \n",
    "    print(\"Test Prediction:\")\n",
    "    print(f\"Text: {test_text[:100]}...\")\n",
    "    print(f\"Prediction: {result['prediction']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"Actual label: {'Good' if val_labels[0] == 1 else 'Bad'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
