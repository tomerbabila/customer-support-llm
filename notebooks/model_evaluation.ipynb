{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed0a8f8",
   "metadata": {},
   "source": [
    "# Model Evaluation & Visualization\n",
    "\n",
    "This notebook provides comprehensive evaluation and visualization of the fine-tuned customer support quality classifier model.\n",
    "\n",
    "**Contents:**\n",
    "- Load the fine-tuned model\n",
    "- Generate predictions on test data\n",
    "- Create confusion matrix and classification report\n",
    "- Visualize performance metrics\n",
    "- ROC and Precision-Recall curves\n",
    "- Feature importance analysis\n",
    "- Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f506d3",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713066d5",
   "metadata": {},
   "source": [
    "## Load the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a44706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the saved model\n",
    "model_path = \"../models/customer-support-classifier-final\"\n",
    "\n",
    "# Check if model exists\n",
    "if not Path(model_path).exists():\n",
    "    print(f\"âŒ Model not found at: {model_path}\")\n",
    "    print(\"Please run the model_finetuning.ipynb notebook first to train the model.\")\n",
    "else:\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    # Move model to appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"âœ… Model loaded successfully from: {model_path}\")\n",
    "    print(f\"Model device: {device}\")\n",
    "    print(f\"Model type: {type(model).__name__}\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabfbb1d",
   "metadata": {},
   "source": [
    "## Load Test Data\n",
    "\n",
    "Load the transcript data for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcripts(folder_path, label):\n",
    "    \"\"\"Load all transcript files from a folder.\"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    if not folder.exists():\n",
    "        print(f\"âš ï¸  Folder not found: {folder_path}\")\n",
    "        return []\n",
    "    \n",
    "    data = []\n",
    "    txt_files = sorted(folder.glob(\"*.txt\"))\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        try:\n",
    "            text = txt_file.read_text(encoding='utf-8').strip()\n",
    "            if text:\n",
    "                data.append({\n",
    "                    'text': text,\n",
    "                    'label': label,\n",
    "                    'filename': txt_file.name\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error reading {txt_file.name}: {e}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load good answers (label=1)\n",
    "good_data = load_transcripts(\"../data/good_answer_transcripts\", label=1)\n",
    "print(f\"âœ… Loaded {len(good_data)} good answers\")\n",
    "\n",
    "# Load bad answers (label=0)\n",
    "bad_data = load_transcripts(\"../data/bad_answer_transcripts\", label=0)\n",
    "print(f\"âŒ Loaded {len(bad_data)} bad answers\")\n",
    "\n",
    "# Combine datasets\n",
    "all_data = good_data + bad_data\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "print(f\"\\nðŸ“Š Total samples: {len(df)}\")\n",
    "print(f\"   Good answers: {len(good_data)} ({len(good_data)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Bad answers:  {len(bad_data)} ({len(bad_data)/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nðŸ“ Sample text lengths:\")\n",
    "print(f\"   Mean: {df['text'].str.len().mean():.0f} characters\")\n",
    "print(f\"   Median: {df['text'].str.len().median():.0f} characters\")\n",
    "print(f\"   Max: {df['text'].str.len().max():.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90876021",
   "metadata": {},
   "source": [
    "## Generate Predictions\n",
    "\n",
    "Run the model on all data and collect predictions, probabilities, and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(texts, batch_size=16):\n",
    "    \"\"\"Generate predictions for a batch of texts.\"\"\"\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    all_logits = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=256\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "        all_probabilities.extend(probs.cpu().numpy())\n",
    "        all_logits.extend(logits.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_probabilities), np.array(all_logits)\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "predictions, probabilities, logits = predict_batch(df['text'].tolist())\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df['prediction'] = predictions\n",
    "df['prob_bad'] = probabilities[:, 0]\n",
    "df['prob_good'] = probabilities[:, 1]\n",
    "df['confidence'] = np.max(probabilities, axis=1)\n",
    "df['correct'] = df['prediction'] == df['label']\n",
    "\n",
    "print(\"\\nâœ… Predictions complete!\")\n",
    "print(f\"Accuracy: {df['correct'].mean():.2%}\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(df['prediction'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46151c",
   "metadata": {},
   "source": [
    "## 1. Confusion Matrix Visualization\n",
    "\n",
    "Visualize the confusion matrix to understand classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eceb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(df['label'], df['prediction'])\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Confusion Matrix (counts)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Bad (0)', 'Good (1)'], \n",
    "            yticklabels=['Bad (0)', 'Good (1)'],\n",
    "            ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 2: Confusion Matrix (normalized)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=['Bad (0)', 'Good (1)'], \n",
    "            yticklabels=['Bad (0)', 'Good (1)'],\n",
    "            ax=axes[1], cbar_kws={'label': 'Percentage'})\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics from confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"\\nðŸ“Š Confusion Matrix Breakdown:\")\n",
    "print(f\"   True Negatives (TN):  {tn} - Correctly identified bad answers\")\n",
    "print(f\"   False Positives (FP): {fp} - Bad answers misclassified as good\")\n",
    "print(f\"   False Negatives (FN): {fn} - Good answers misclassified as bad\")\n",
    "print(f\"   True Positives (TP):  {tp} - Correctly identified good answers\")\n",
    "print(f\"\\n   Sensitivity (TPR): {tp/(tp+fn):.2%} - % of good answers correctly identified\")\n",
    "print(f\"   Specificity (TNR): {tn/(tn+fp):.2%} - % of bad answers correctly identified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a1e23",
   "metadata": {},
   "source": [
    "## 2. Classification Metrics Dashboard\n",
    "\n",
    "Display comprehensive classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8703df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "accuracy = accuracy_score(df['label'], df['prediction'])\n",
    "precision = precision_score(df['label'], df['prediction'])\n",
    "recall = recall_score(df['label'], df['prediction'])\n",
    "f1 = f1_score(df['label'], df['prediction'])\n",
    "\n",
    "# Create classification report\n",
    "report = classification_report(df['label'], df['prediction'], \n",
    "                               target_names=['Bad Answer', 'Good Answer'],\n",
    "                               output_dict=True)\n",
    "\n",
    "# Visualize metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Main metrics bar chart\n",
    "metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "}\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "bars = axes[0, 0].bar(metrics.keys(), metrics.values(), color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_ylim([0, 1.0])\n",
    "axes[0, 0].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Overall Classification Metrics', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.3f}',\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Per-class metrics\n",
    "classes = ['Bad Answer', 'Good Answer']\n",
    "class_precision = [report['Bad Answer']['precision'], report['Good Answer']['precision']]\n",
    "class_recall = [report['Bad Answer']['recall'], report['Good Answer']['recall']]\n",
    "class_f1 = [report['Bad Answer']['f1-score'], report['Good Answer']['f1-score']]\n",
    "\n",
    "x = np.arange(len(classes))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = axes[0, 1].bar(x - width, class_precision, width, label='Precision', color='#3498db', alpha=0.7)\n",
    "bars2 = axes[0, 1].bar(x, class_recall, width, label='Recall', color='#2ecc71', alpha=0.7)\n",
    "bars3 = axes[0, 1].bar(x + width, class_f1, width, label='F1-Score', color='#f39c12', alpha=0.7)\n",
    "\n",
    "axes[0, 1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Per-Class Metrics', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(classes)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_ylim([0, 1.0])\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Confidence distribution by correctness\n",
    "correct_conf = df[df['correct']]['confidence']\n",
    "incorrect_conf = df[~df['correct']]['confidence']\n",
    "\n",
    "axes[1, 0].hist(correct_conf, bins=20, alpha=0.7, label='Correct', color='green', edgecolor='black')\n",
    "axes[1, 0].hist(incorrect_conf, bins=20, alpha=0.7, label='Incorrect', color='red', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Confidence', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Support (sample count) per class\n",
    "support = [report['Bad Answer']['support'], report['Good Answer']['support']]\n",
    "bars = axes[1, 1].bar(classes, support, color=['#e74c3c', '#2ecc71'], alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Class Distribution (Support)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height)}',\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(df['label'], df['prediction'], \n",
    "                           target_names=['Bad Answer', 'Good Answer']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5defa93",
   "metadata": {},
   "source": [
    "## 3. ROC Curve and AUC Score\n",
    "\n",
    "Plot the Receiver Operating Characteristic curve and calculate Area Under the Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(df['label'], df['prob_good'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Calculate Precision-Recall curve\n",
    "precision_curve, recall_curve, thresholds_pr = precision_recall_curve(df['label'], df['prob_good'])\n",
    "avg_precision = average_precision_score(df['label'], df['prob_good'])\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: ROC Curve\n",
    "axes[0].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "axes[0].set_xlim([0.0, 1.0])\n",
    "axes[0].set_ylim([0.0, 1.05])\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc=\"lower right\", fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Add optimal threshold point\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds_roc[optimal_idx]\n",
    "axes[0].scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='red', \n",
    "                s=100, label=f'Optimal threshold = {optimal_threshold:.3f}', zorder=5)\n",
    "axes[0].legend(loc=\"lower right\", fontsize=10)\n",
    "\n",
    "# Plot 2: Precision-Recall Curve\n",
    "axes[1].plot(recall_curve, precision_curve, color='green', lw=2,\n",
    "             label=f'PR curve (AP = {avg_precision:.3f})')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc=\"lower left\", fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Add F1 iso-curves\n",
    "f_scores = np.linspace(0.2, 0.9, num=4)\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    axes[1].plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.3, linestyle='--', lw=1)\n",
    "    axes[1].annotate(f'F1={f_score:.1f}', xy=(0.9, y[45] + 0.02), fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š ROC Analysis:\")\n",
    "print(f\"   AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"   Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"   TPR at optimal: {tpr[optimal_idx]:.4f}\")\n",
    "print(f\"   FPR at optimal: {fpr[optimal_idx]:.4f}\")\n",
    "print(f\"\\nðŸ“Š Precision-Recall Analysis:\")\n",
    "print(f\"   Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"   Max F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab436cf0",
   "metadata": {},
   "source": [
    "## 4. Probability Distribution Analysis\n",
    "\n",
    "Analyze the distribution of predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Probability distributions by true label\n",
    "axes[0, 0].hist(df[df['label']==0]['prob_good'], bins=30, alpha=0.6, \n",
    "                label='True Bad Answers', color='red', edgecolor='black')\n",
    "axes[0, 0].hist(df[df['label']==1]['prob_good'], bins=30, alpha=0.6, \n",
    "                label='True Good Answers', color='green', edgecolor='black')\n",
    "axes[0, 0].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "axes[0, 0].set_xlabel('Predicted Probability (Good)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Probability Distribution by True Label', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Scatter plot of probabilities\n",
    "colors = ['red' if label == 0 else 'green' for label in df['label']]\n",
    "markers = ['x' if not correct else 'o' for correct in df['correct']]\n",
    "\n",
    "for i, (prob_bad, prob_good, color, marker, correct) in enumerate(zip(\n",
    "    df['prob_bad'], df['prob_good'], colors, markers, df['correct'])):\n",
    "    axes[0, 1].scatter(prob_bad, prob_good, c=color, marker=marker, \n",
    "                      alpha=0.6, s=50,\n",
    "                      label='Correct' if i == 0 and correct else ('Incorrect' if i == 0 else None))\n",
    "\n",
    "axes[0, 1].plot([0, 1], [1, 0], 'k--', linewidth=2, label='Decision Boundary')\n",
    "axes[0, 1].set_xlabel('P(Bad Answer)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('P(Good Answer)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Probability Space', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].set_xlim([0, 1])\n",
    "axes[0, 1].set_ylim([0, 1])\n",
    "\n",
    "# Create custom legend\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='g', markersize=8, label='True Good'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='r', markersize=8, label='True Bad'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', markersize=8, label='Correct'),\n",
    "    Line2D([0], [0], marker='x', color='w', markerfacecolor='gray', markersize=8, label='Incorrect')\n",
    "]\n",
    "axes[0, 1].legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# Plot 3: Confidence by prediction outcome\n",
    "data_to_plot = [\n",
    "    df[(df['label']==0) & (df['prediction']==0)]['confidence'],  # True Negative\n",
    "    df[(df['label']==0) & (df['prediction']==1)]['confidence'],  # False Positive\n",
    "    df[(df['label']==1) & (df['prediction']==0)]['confidence'],  # False Negative\n",
    "    df[(df['label']==1) & (df['prediction']==1)]['confidence'],  # True Positive\n",
    "]\n",
    "labels = ['True\\nNegative', 'False\\nPositive', 'False\\nNegative', 'True\\nPositive']\n",
    "colors_box = ['lightblue', 'lightcoral', 'lightyellow', 'lightgreen']\n",
    "\n",
    "bp = axes[1, 0].boxplot(data_to_plot, labels=labels, patch_artist=True,\n",
    "                        showmeans=True, meanline=True)\n",
    "\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "axes[1, 0].set_ylabel('Confidence Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Confidence Distribution by Prediction Type', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "\n",
    "# Plot 4: Calibration analysis (binned)\n",
    "n_bins = 10\n",
    "bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "bin_accuracies = []\n",
    "bin_counts = []\n",
    "\n",
    "for i in range(n_bins):\n",
    "    mask = (df['prob_good'] >= bin_edges[i]) & (df['prob_good'] < bin_edges[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        bin_accuracies.append(df[mask]['correct'].mean())\n",
    "        bin_counts.append(mask.sum())\n",
    "    else:\n",
    "        bin_accuracies.append(0)\n",
    "        bin_counts.append(0)\n",
    "\n",
    "axes[1, 1].bar(bin_centers, bin_accuracies, width=0.08, alpha=0.7, \n",
    "               edgecolor='black', label='Empirical Accuracy')\n",
    "axes[1, 1].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Calibration')\n",
    "axes[1, 1].set_xlabel('Predicted Probability (Good)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Empirical Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Model Calibration', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].set_xlim([0, 1])\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print calibration statistics\n",
    "print(\"\\nðŸ“Š Calibration Statistics:\")\n",
    "print(f\"   Mean confidence (correct): {df[df['correct']]['confidence'].mean():.4f}\")\n",
    "print(f\"   Mean confidence (incorrect): {df[~df['correct']]['confidence'].mean():.4f}\")\n",
    "print(f\"   Samples with >90% confidence: {(df['confidence'] > 0.9).sum()} ({(df['confidence'] > 0.9).mean():.1%})\")\n",
    "print(f\"   Accuracy for >90% confidence: {df[df['confidence'] > 0.9]['correct'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7233a2f8",
   "metadata": {},
   "source": [
    "## 5. Error Analysis\n",
    "\n",
    "Analyze misclassified examples to understand model weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47126cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get misclassified examples\n",
    "errors_df = df[~df['correct']].copy()\n",
    "errors_df = errors_df.sort_values('confidence', ascending=False)\n",
    "\n",
    "print(f\"ðŸ“Š Error Analysis Summary:\")\n",
    "print(f\"   Total errors: {len(errors_df)}\")\n",
    "print(f\"   False Positives (Bad â†’ Good): {((errors_df['label']==0) & (errors_df['prediction']==1)).sum()}\")\n",
    "print(f\"   False Negatives (Good â†’ Bad): {((errors_df['label']==1) & (errors_df['prediction']==0)).sum()}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Error rate by text length\n",
    "df['text_length'] = df['text'].str.len()\n",
    "length_bins = pd.qcut(df['text_length'], q=5, duplicates='drop')\n",
    "error_by_length = df.groupby(length_bins)['correct'].agg(['mean', 'count'])\n",
    "error_by_length['error_rate'] = 1 - error_by_length['mean']\n",
    "\n",
    "x_labels = [f\"{int(interval.left)}-{int(interval.right)}\" for interval in error_by_length.index]\n",
    "axes[0, 0].bar(range(len(error_by_length)), error_by_length['error_rate'], \n",
    "               color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xticks(range(len(error_by_length)))\n",
    "axes[0, 0].set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "axes[0, 0].set_xlabel('Text Length (characters)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Error Rate', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Error Rate by Text Length', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, (idx, row) in enumerate(error_by_length.iterrows()):\n",
    "    axes[0, 0].text(i, row['error_rate'] + 0.01, f\"n={int(row['count'])}\", \n",
    "                   ha='center', fontsize=9)\n",
    "\n",
    "# Plot 2: Error types\n",
    "error_types = pd.DataFrame({\n",
    "    'Error Type': ['False Positive\\n(Bad â†’ Good)', 'False Negative\\n(Good â†’ Bad)'],\n",
    "    'Count': [\n",
    "        ((errors_df['label']==0) & (errors_df['prediction']==1)).sum(),\n",
    "        ((errors_df['label']==1) & (errors_df['prediction']==0)).sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "bars = axes[0, 1].bar(error_types['Error Type'], error_types['Count'], \n",
    "                     color=['#e74c3c', '#3498db'], alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_ylabel('Number of Errors', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Error Types Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{int(height)}',\n",
    "                   ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Plot 3: Confidence of errors vs correct predictions\n",
    "axes[1, 0].hist(df[df['correct']]['confidence'], bins=20, alpha=0.6, \n",
    "               label=f'Correct (n={df[\"correct\"].sum()})', color='green', edgecolor='black')\n",
    "axes[1, 0].hist(errors_df['confidence'], bins=20, alpha=0.6, \n",
    "               label=f'Errors (n={len(errors_df)})', color='red', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Confidence', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Confidence Distribution: Correct vs Errors', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: High-confidence errors\n",
    "high_conf_errors = errors_df[errors_df['confidence'] > 0.8]\n",
    "axes[1, 1].axis('off')\n",
    "axes[1, 1].text(0.5, 0.95, 'High-Confidence Errors (>80%)', \n",
    "               ha='center', va='top', fontsize=14, fontweight='bold',\n",
    "               transform=axes[1, 1].transAxes)\n",
    "\n",
    "error_summary = f\"\"\"\n",
    "Total high-confidence errors: {len(high_conf_errors)}\n",
    "\n",
    "False Positives (Bad â†’ Good): {((high_conf_errors['label']==0) & (high_conf_errors['prediction']==1)).sum()}\n",
    "  â€¢ Model predicted \"good\" with high confidence\n",
    "  â€¢ But actual label was \"bad\"\n",
    "  \n",
    "False Negatives (Good â†’ Bad): {((high_conf_errors['label']==1) & (high_conf_errors['prediction']==0)).sum()}\n",
    "  â€¢ Model predicted \"bad\" with high confidence\n",
    "  â€¢ But actual label was \"good\"\n",
    "\n",
    "Average confidence of errors: {errors_df['confidence'].mean():.2%}\n",
    "Median confidence of errors: {errors_df['confidence'].median():.2%}\n",
    "\n",
    "This suggests the model is {\"well-calibrated\" if errors_df['confidence'].mean() < 0.7 else \"overconfident\"}\n",
    "on misclassified examples.\n",
    "\"\"\"\n",
    "\n",
    "axes[1, 1].text(0.05, 0.85, error_summary, \n",
    "               ha='left', va='top', fontsize=11, family='monospace',\n",
    "               transform=axes[1, 1].transAxes,\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show examples of high-confidence errors\n",
    "if len(high_conf_errors) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"HIGH-CONFIDENCE ERRORS (Top 3)\")\n",
    "    print(\"=\"*60)\n",
    "    for idx, (i, row) in enumerate(high_conf_errors.head(3).iterrows(), 1):\n",
    "        true_label = \"Good\" if row['label'] == 1 else \"Bad\"\n",
    "        pred_label = \"Good\" if row['prediction'] == 1 else \"Bad\"\n",
    "        print(f\"\\n[Error #{idx}] File: {row['filename']}\")\n",
    "        print(f\"True Label: {true_label} | Predicted: {pred_label} | Confidence: {row['confidence']:.2%}\")\n",
    "        print(f\"Text preview: {row['text'][:200]}...\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf160f1",
   "metadata": {},
   "source": [
    "## 6. Performance Summary Dashboard\n",
    "\n",
    "Create a comprehensive summary of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics\n",
    "summary_stats = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'Avg Precision'],\n",
    "    'Score': [\n",
    "        accuracy_score(df['label'], df['prediction']),\n",
    "        precision_score(df['label'], df['prediction']),\n",
    "        recall_score(df['label'], df['prediction']),\n",
    "        f1_score(df['label'], df['prediction']),\n",
    "        roc_auc,\n",
    "        avg_precision\n",
    "    ]\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Main metrics comparison (spans 2 columns)\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "colors_metrics = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12', '#9b59b6', '#1abc9c']\n",
    "bars = ax1.barh(summary_df['Metric'], summary_df['Score'], color=colors_metrics, alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlim([0, 1.0])\n",
    "ax1.set_xlabel('Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Model Performance Metrics Overview', fontsize=16, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, (bar, score) in enumerate(zip(bars, summary_df['Score'])):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "            f'{score:.4f}',\n",
    "            ha='left', va='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Plot 2: Class balance pie chart\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "class_counts = df['label'].value_counts()\n",
    "ax2.pie(class_counts, labels=['Good Answers', 'Bad Answers'], autopct='%1.1f%%',\n",
    "       colors=['#2ecc71', '#e74c3c'], startangle=90, explode=(0.05, 0.05))\n",
    "ax2.set_title('Dataset Balance', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 3: Confusion matrix heatmap\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "cm_normalized = confusion_matrix(df['label'], df['prediction'], normalize='true')\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn', \n",
    "           xticklabels=['Bad', 'Good'], yticklabels=['Bad', 'Good'],\n",
    "           ax=ax3, cbar_kws={'label': 'Rate'}, vmin=0, vmax=1)\n",
    "ax3.set_xlabel('Predicted', fontweight='bold')\n",
    "ax3.set_ylabel('Actual', fontweight='bold')\n",
    "ax3.set_title('Normalized Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 4: ROC curve (mini version)\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "ax4.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.3f}')\n",
    "ax4.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax4.fill_between(fpr, tpr, alpha=0.2, color='orange')\n",
    "ax4.set_xlabel('FPR', fontweight='bold')\n",
    "ax4.set_ylabel('TPR', fontweight='bold')\n",
    "ax4.set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "ax4.legend(loc='lower right')\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "# Plot 5: Prediction distribution\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "pred_dist = df.groupby(['label', 'prediction']).size().unstack(fill_value=0)\n",
    "pred_dist.plot(kind='bar', ax=ax5, color=['#e74c3c', '#2ecc71'], alpha=0.7, edgecolor='black')\n",
    "ax5.set_xlabel('True Label', fontweight='bold')\n",
    "ax5.set_ylabel('Count', fontweight='bold')\n",
    "ax5.set_title('Prediction Distribution', fontsize=12, fontweight='bold')\n",
    "ax5.set_xticklabels(['Bad (0)', 'Good (1)'], rotation=0)\n",
    "ax5.legend(['Predicted Bad', 'Predicted Good'], loc='upper left')\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 6-8: Key statistics (bottom row)\n",
    "stats_data = [\n",
    "    ('Total Samples', len(df), '#3498db'),\n",
    "    ('Correct Predictions', df['correct'].sum(), '#2ecc71'),\n",
    "    ('Errors', (~df['correct']).sum(), '#e74c3c'),\n",
    "    ('Mean Confidence', f\"{df['confidence'].mean():.2%}\", '#f39c12'),\n",
    "    ('Good Samples', (df['label']==1).sum(), '#2ecc71'),\n",
    "    ('Bad Samples', (df['label']==0).sum(), '#e74c3c'),\n",
    "]\n",
    "\n",
    "for i, (label, value, color) in enumerate(stats_data):\n",
    "    ax = fig.add_subplot(gs[2, i // 2])\n",
    "    ax.text(0.5, 0.5, str(value), ha='center', va='center', \n",
    "           fontsize=32, fontweight='bold', color=color)\n",
    "    ax.text(0.5, 0.2, label, ha='center', va='center',\n",
    "           fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    # Add box around stat\n",
    "    from matplotlib.patches import Rectangle\n",
    "    rect = Rectangle((0.05, 0.05), 0.9, 0.9, linewidth=2, \n",
    "                     edgecolor=color, facecolor='none', alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    if i == 1:  # After two stats, move to next row\n",
    "        break\n",
    "\n",
    "# Adjust the remaining stats\n",
    "for i, (label, value, color) in enumerate(stats_data[2:], 0):\n",
    "    ax = fig.add_subplot(gs[2, i])\n",
    "    ax.text(0.5, 0.5, str(value), ha='center', va='center', \n",
    "           fontsize=32, fontweight='bold', color=color)\n",
    "    ax.text(0.5, 0.2, label, ha='center', va='center',\n",
    "           fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    from matplotlib.patches import Rectangle\n",
    "    rect = Rectangle((0.05, 0.05), 0.9, 0.9, linewidth=2, \n",
    "                     edgecolor=color, facecolor='none', alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.suptitle('Customer Support Quality Classifier - Performance Dashboard', \n",
    "            fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸ“Š Overall Performance:\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   Precision: {precision:.4f} (of predicted good, {precision*100:.2f}% are actually good)\")\n",
    "print(f\"   Recall:    {recall:.4f} ({recall*100:.2f}% of actual good answers detected)\")\n",
    "print(f\"   F1-Score:  {f1:.4f}\")\n",
    "print(f\"   AUC-ROC:   {roc_auc:.4f}\")\n",
    "print(f\"\\nðŸ“ˆ Dataset Info:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   Good answers:  {(df['label']==1).sum()} ({(df['label']==1).mean()*100:.1f}%)\")\n",
    "print(f\"   Bad answers:   {(df['label']==0).sum()} ({(df['label']==0).mean()*100:.1f}%)\")\n",
    "print(f\"\\nâœ… Predictions:\")\n",
    "print(f\"   Correct:   {df['correct'].sum()} ({df['correct'].mean()*100:.2f}%)\")\n",
    "print(f\"   Incorrect: {(~df['correct']).sum()} ({(~df['correct']).mean()*100:.2f}%)\")\n",
    "print(f\"\\nðŸŽ¯ Confidence:\")\n",
    "print(f\"   Mean confidence: {df['confidence'].mean():.2%}\")\n",
    "print(f\"   High confidence (>90%): {(df['confidence'] > 0.9).sum()} samples\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
